{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Project 3: Open Street Map\n",
    "### Author: Denis Rudolf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some words about the data. I downloaded an OSM XML file where a part of Western Germany is mapped (Düsseldorf area, latitude between 51.16696° and 51.23432°, longitude between 6.64838° and  6.87130°). The OSM file size is 230.532 MB. I would like to work with more data but for the auditing and cleaning step with xml.etree.cElementTree my RAM of 8 GB is not sufficient.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Auditing and cleaning of the data\n",
    "\n",
    "First, I checked street names for non-alphanumeric strings. I found some street names with non-alphanumeric values but most of them were correct (validated by Internet research). I corrected the incorrect ones on OSM. Then, I audited the postcodes and found them to be correct by cross-checking with Google maps. Then, I audited and cleaned the phone numbers putting them all in the same format (example: +4921311520). I also created a contact dictionary with the keys phone, fax and website. I also set all the values for the key = wheelchair to be lower case. Furthermore, I noticed that there is a key = fixme or key = FIXME (see program output, partly in German), but this has to be addressed for each case separately.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "\n",
    "OSMFILE = \"map_Duesseldorf_Neuss.osm\"\n",
    "\n",
    "problemchars_streetname = re.compile(r'[=\\+\\&<>;\\\"\\?%#$@\\,\\t\\r\\n]', re.IGNORECASE)\n",
    "problemchars_phone = re.compile(r'[=\\/&<>;\\'\"\\?%#$@\\,\\.\\t\\r\\n]', re.IGNORECASE)\n",
    "phone_re = re.compile(r\"\\+49|0049\")\n",
    "phone_re_0049 = re.compile(r\"0049\")\n",
    "non_digits_re = re.compile(r\"\\D\")\n",
    "fixme_re = re.compile(r'fixme', re.IGNORECASE)\n",
    "fixme_list =[]\n",
    "\n",
    "def audit_street(street_name):\n",
    "    m = problemchars_streetname.search(street_name)\n",
    "    if m:\n",
    "        print u\"Problem with the street name: {}\".format(street_name)\n",
    "    return street_name    \n",
    "\n",
    "def audit_postcode(postcode): \n",
    "    postcode = int(postcode)\n",
    "    if postcode > 41564 or postcode < 40210:\n",
    "        print u\"Problem with the postcode: {}\".format(postcode)   \n",
    "    return postcode\n",
    "\n",
    "def audit_is_wheelchair(is_wheelchair):\n",
    "    mapping = {\"Yes\":\"yes\", \"No\":\"no\", \"Limited\": \"limited\"}\n",
    "    if set([is_wheelchair]) < set([\"Yes\", \"No\", \"Limited\"]):\n",
    "        return mapping[is_wheelchair]\n",
    "    else:\n",
    "        return is_wheelchair\n",
    "\n",
    "def audit_phone_number(phone): \n",
    "    # remove hyphons\n",
    "    phone = \"\".join(phone.split(\"-\")[:])\n",
    "    # remove white spaces\n",
    "    phone = \"\".join(phone.split()[:])\n",
    "    # remove slashes\n",
    "    phone = \"\".join(phone.split(r\"/\")[:])\n",
    "    # check if the country code is there\n",
    "    if phone_re.search(phone):\n",
    "        # take only the first phone number if there are more than one\n",
    "        m = problemchars_phone.search(phone)\n",
    "        if m:\n",
    "            char = m.group()\n",
    "          #  print \"Problem character: \" + char\n",
    "            return phone.split(char)[0]\n",
    "        elif phone_re_0049.search(phone):\n",
    "            return \"+49\"+ phone.strip(\"0049\")\n",
    "        else:\n",
    "            return phone\n",
    "    elif non_digits_re.search(phone):\n",
    "        return None\n",
    "    else:\n",
    "        return \"+49\" + phone.strip(\"0\")\n",
    "\n",
    "def audit_fixme(fixme):\n",
    "    fixme_list.append(fixme)\n",
    "    \n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if tag.attrib['k'] == \"addr:street\":\n",
    "                    audit_street(tag.attrib['v'])\n",
    "                if tag.attrib['k'] == \"addr:postcode\":    \n",
    "                    audit_postcode(tag.attrib['v'])\n",
    "                if tag.attrib['k'] == \"phone\":\n",
    "                    audit_phone_number(tag.attrib['v']) \n",
    "                if fixme_re.search(tag.attrib['k']):\n",
    "                    audit_fixme(tag.attrib['v'])    \n",
    "    osm_file.close()\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    audit(OSMFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of key=fixme or key=FIXME is 1672.\n",
      "Here are 10 examples: \n",
      "\n",
      "['Exact position - are the connections correct?',\n",
      " 'name of exit',\n",
      " 'exact position',\n",
      " 'Warnton?',\n",
      " u'zu busrelationen hinzuf\\xfcgen',\n",
      " 'name of exit',\n",
      " 'Am Bahnsteigdach befestigt',\n",
      " u'Welche Gastst\\xe4tten sind da aktuell?',\n",
      " 'auch Bushaltestelle?',\n",
      " 'opening_hours']\n"
     ]
    }
   ],
   "source": [
    "print 'The number of key=fixme or key=FIXME is {}.'.format(len(fixme_list))\n",
    "print 'Here are 10 examples: \\n'\n",
    "pprint.pprint(fixme_list[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is for data auditing, cleaning, inserting into the data model and to store the data in the json file. It is build upon the Udacity excercise in the case study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\&<>;\\'\"\\?%#$@\\,\\.\\t\\r\\n]')\n",
    "\n",
    "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "\n",
    "\n",
    "def shape_element(element):\n",
    "    node = {}\n",
    "    if element.tag == \"node\" or element.tag == \"way\" :\n",
    "        # get the list of keys of the \"node\"/\"way\" dict\n",
    "        keys = element.attrib.keys()\n",
    "        # define the sub-dictionary \n",
    "        dic_created = {}\n",
    "        dic_address = {}\n",
    "        dic_contact = {}\n",
    "        dic_created.fromkeys(CREATED)\n",
    "        _type = element.tag\n",
    "        # check the existence of the keys\n",
    "        if set([\"id\", \"changeset\",\"user\",\"version\",\"uid\",\"timestamp\"]) <= set(keys):\n",
    "           # print element.attrib\n",
    "            _id = element.attrib[\"id\"]\n",
    "            dic_created[\"changeset\"] = element.attrib[\"changeset\"]\n",
    "            dic_created[\"user\"] = element.attrib[\"user\"]\n",
    "            dic_created[\"version\"] = element.attrib[\"version\"]\n",
    "            dic_created[\"uid\"] = element.attrib[\"uid\"]\n",
    "            dic_created[\"timestamp\"] = element.attrib[\"timestamp\"]\n",
    "            node = {\"id\":_id, \"general_type\": _type, \"created\": dic_created }\n",
    "        if set([\"lat\", \"lon\"]) <= set(keys):\n",
    "            lat = float(element.attrib[\"lat\"])\n",
    "            lon = float(element.attrib[\"lon\"])\n",
    "            node[\"pos\"] = [lon,lat]\n",
    "        # for ways: make a list of node refs\n",
    "        if element.tag == \"way\":   \n",
    "            node_refs = []\n",
    "            for tag in element.iter(\"nd\"):\n",
    "               # print \"Key: {}, Value: {}\".format(tag.attrib[\"k\"],tag.attrib[\"v\"])  \n",
    "               # print tag.attrib[\"ref\"]\n",
    "                node_refs.append(tag.attrib[\"ref\"]) \n",
    "            node[\"node_refs\"] = node_refs\n",
    "        # iterate over the tags\n",
    "        for tag in element.iter(\"tag\"):\n",
    "            # select tags with one colon  \n",
    "            if lower_colon.search(tag.attrib[\"k\"]):\n",
    "                colon_list = tag.attrib[\"k\"].split(\":\")\n",
    "                # create an address dict \n",
    "                if colon_list[0] == \"addr\":\n",
    "                    address_type = colon_list[1]\n",
    "                    # audit and clean the street name if necessary\n",
    "                    if address_type == \"street\":\n",
    "                        address_value = audit_street(tag.attrib[\"v\"])\n",
    "                    # audit and clean the postcode if necessary    \n",
    "                    elif address_type == \"postcode\":  \n",
    "                        address_value = audit_postcode(tag.attrib[\"v\"])\n",
    "                    else:\n",
    "                        address_value = tag.attrib[\"v\"]\n",
    "                    # ignore values with problematic characters\n",
    "                    # if not is_problemchars(address_value):\n",
    "                    dic_address[address_type] = address_value\n",
    "                # create a contact dict\n",
    "                elif colon_list[0] == \"contact\":\n",
    "                    contact_type = colon_list[1]\n",
    "                    # audit and clean the street name if necessary\n",
    "                    if contact_type == \"phone\":\n",
    "                        contact_value = audit_phone_number(tag.attrib[\"v\"])\n",
    "                    # audit and clean the postcode if necessary    \n",
    "                    elif contact_type == \"fax\":  \n",
    "                        contact_value = audit_phone_number(tag.attrib[\"v\"])\n",
    "                    elif contact_type == \"website\":  \n",
    "                        contact_value = tag.attrib[\"v\"]\n",
    "                    elif contact_type == \"email\":  \n",
    "                        contact_value = tag.attrib[\"v\"]    \n",
    "                    else:\n",
    "                        contact_value = tag.attrib[\"v\"]\n",
    "                    # ignore values with problematic characters\n",
    "                    # if not is_problemchars(address_value):\n",
    "                    dic_contact[contact_type] = contact_value\n",
    "                # other cases with colon but without \"addr\"\n",
    "                else:                   \n",
    "                    value = tag.attrib[\"v\"]\n",
    "                    # ignore values with problematic characters\n",
    "                    if not is_problemchars(value):\n",
    "                        s = \" \"\n",
    "                        key_string = s.join(colon_list)   \n",
    "                        node[key_string] = value                    \n",
    "            # select tags with lower case \n",
    "            if lower.search(tag.attrib[\"k\"]): \n",
    "                key = tag.attrib[\"k\"]\n",
    "                value = tag.attrib[\"v\"]\n",
    "                # ignore values with problematic characters\n",
    "                if not is_problemchars(value):\n",
    "                    if key == \"phone\":\n",
    "                        # print key\n",
    "                        node[key] = audit_phone_number(value)\n",
    "                    elif key == \"wheelchair\":\n",
    "                        node[key] = audit_is_wheelchair(value)\n",
    "                    else:\n",
    "                        node[key] = value  \n",
    "            # print problematic characters\n",
    "            if is_problemchars(tag.attrib[\"v\"]):\n",
    "               # print tag.attrib[\"v\"]\n",
    "                pass\n",
    "        # insert the address and contact dict into the node dict\n",
    "        if dic_address:\n",
    "            node[\"address\"] = dic_address\n",
    "        if dic_contact:\n",
    "            node[\"contact\"] = dic_contact\n",
    "       \n",
    "        return node\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def is_problemchars(string):\n",
    "    return bool(problemchars.search(string))\n",
    "\n",
    "def process_map(file_in, pretty = False):\n",
    "    # You do not need to change this file\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    data = []\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                data.append(el)\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")\n",
    "    return data\n",
    "\n",
    "def test():\n",
    "    # NOTE: if you are running this code on your computer, with a larger dataset, \n",
    "    # call the process_map procedure with pretty=False. The pretty=True option adds \n",
    "    # additional spaces to the output, making it significantly larger.\n",
    "    data = process_map('map_Duesseldorf_Neuss.osm')\n",
    "    print \"Number of dictionaries: {}\".format(len(data))\n",
    "    pprint.pprint(data[0:9])\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Inserting the data into Mongo DB and querying the database\n",
    "\n",
    "I inserted the json file 'map_Duesseldorf_Neuss.osm.json' in the database OSM as a collection map_Duesseldorf_Neuss.\n",
    "Let's have a look at the number of documents, nodes and ways first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of documents is 971624.\n",
      "The number of nodes is 817709.\n",
      "The number of ways is 153915.\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import pprint\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "client = MongoClient('localhost:27017')\n",
    "db = client.OSM\n",
    "# some simple queries\n",
    "size = db.map_Duesseldorf_Neuss.find().count()\n",
    "print \"The number of documents is {}.\".format(size)\n",
    "num_nodes = db.map_Duesseldorf_Neuss.find({\"general_type\": \"node\"}).count()\n",
    "print \"The number of nodes is {}.\".format(num_nodes)\n",
    "num_ways = db.map_Duesseldorf_Neuss.find({\"general_type\": \"way\"}).count()\n",
    "print \"The number of ways is {}.\".format(num_ways)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's count the number if users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique users is 1009.\n"
     ]
    }
   ],
   "source": [
    "pipeline = [{'$group': { '_id': '$created.user',}}\n",
    "            ]\n",
    "result_list = []\n",
    "result = db.map_Duesseldorf_Neuss.aggregate(pipeline)\n",
    "while result.alive == True:\n",
    "    result_list.append(result.next())\n",
    "print 'The number of unique users is {}.'.format(len(result_list))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who are the most contributing users?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'_id': u'black_bike', u'count': 262745},\n",
       " {u'_id': u'EinKonstanzer', u'count': 102627},\n",
       " {u'_id': u'rurseekatze', u'count': 93824},\n",
       " {u'_id': u'Antikalk', u'count': 92611},\n",
       " {u'_id': u'rabenkind', u'count': 91562},\n",
       " {u'_id': u'Sharlin', u'count': 55791},\n",
       " {u'_id': u'Athemis', u'count': 43618},\n",
       " {u'_id': u'j-e-d', u'count': 27520},\n",
       " {u'_id': u'mighty_eighty', u'count': 23296},\n",
       " {u'_id': u'Zyras', u'count': 21703}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = [{'$group': { '_id': '$created.user',\n",
    "                              'count': {'$sum': 1 }  }},\n",
    "            \n",
    "                {'$sort': {'count': -1}}]\n",
    "result = db.map_Duesseldorf_Neuss.aggregate(pipeline)\n",
    "[result.next() for i in range(0,10)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These were absolute numbers. I'm also interested in the percentage of contributed documents per user.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': u'black_bike', u'ratio': 27.041839229990202},\n",
      " {u'_id': u'EinKonstanzer', u'ratio': 10.562419207430034},\n",
      " {u'_id': u'rurseekatze', u'ratio': 9.65641029863404},\n",
      " {u'_id': u'Antikalk', u'ratio': 9.531567766955119},\n",
      " {u'_id': u'rabenkind', u'ratio': 9.42360419256832},\n",
      " {u'_id': u'Sharlin', u'ratio': 5.742036013931315},\n",
      " {u'_id': u'Athemis', u'ratio': 4.489185116876487},\n",
      " {u'_id': u'j-e-d', u'ratio': 2.8323713699949775},\n",
      " {u'_id': u'mighty_eighty', u'ratio': 2.3976352992515624},\n",
      " {u'_id': u'Zyras', u'ratio': 2.233682988481141}]\n",
      "\n",
      "Top 10 users contribute 83.91 % to the OSM for the Neuss/Duesseldorf area.\n"
     ]
    }
   ],
   "source": [
    "pipeline = [{'$group': { '_id': '$created.user',\n",
    "                              'count': {'$sum': 1 }  }},\n",
    "             {'$project': {'ratio': {'$divide':['$count', 1e-2*size]}}},\n",
    "            \n",
    "                {'$sort': {'ratio': -1}}]\n",
    "result = db.map_Duesseldorf_Neuss.aggregate(pipeline)\n",
    "result_list = [result.next() for i in range(0,10)] \n",
    "pprint.pprint(result_list)\n",
    "\n",
    "elem_sum = 0\n",
    "for elem in result_list:\n",
    "    elem_sum = elem_sum + elem['ratio']\n",
    "print '\\n Top 10 users contribute {0:.2f} % to the OSM for the Neuss/Duesseldorf area.'.format(elem_sum)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conclusion here is similar to the sample MongoDB project that top 10 users contribute 83.91 % of the documents. Because I'm a selfish person I will also query for the number of my contributions (it could be more).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'count': 7, u'_id': u'DenisRudolf'}\n"
     ]
    }
   ],
   "source": [
    "pipeline = [{'$group': { '_id': '$created.user',\n",
    "                              'count': {'$sum': 1 }  }},\n",
    "            {'$match': { '_id': 'DenisRudolf'}}\n",
    "            ]\n",
    "result = db.map_Duesseldorf_Neuss.aggregate(pipeline)\n",
    "while result.alive == True:\n",
    "    print result.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I'm constructing the geospatial index (2d sphere) in order to be able to make a query with the '$geoNear' operator. I'm interested in the number of key = fixme or key = FIXME being close to our house (lat 51.22085, lon 6.65236), i.e. within a radius of about 10 km (maxDistance = 1.5e-3 rad). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'pos_2dsphere'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymongo import GEOSPHERE\n",
    "from bson.son import SON\n",
    "db.map_Duesseldorf_Neuss.create_index([(\"pos\", GEOSPHERE)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of dictionaries with key = fixme or key = FIXME is 100.\n"
     ]
    }
   ],
   "source": [
    "# our house\n",
    "coord = [6.65236,51.22085]\n",
    "pipeline = [{'$geoNear':{ 'distanceField': 'pos','near': coord, 'spherical': True, \n",
    "                             'query': { 'fixme': {'$exists': 1}}, 'maxDistance': 1.5e-3, \n",
    "                             'distanceField': 'dist.calculated'}}\n",
    "            ]\n",
    "result_list = []\n",
    "result = db.map_Duesseldorf_Neuss.aggregate(pipeline)\n",
    "while result.alive == True:\n",
    "    result_list.append(result.next())\n",
    "print 'The number of dictionaries with key = fixme or key = FIXME is {}.'.format(len(result_list))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I want to retrieve the minimum and maximum longitude and latitude coordinates and to calculate the area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max value 51.23432.\n",
      "The min value 51.16696.\n",
      "The max value 6.87130.\n",
      "The min value 6.64838.\n"
     ]
    }
   ],
   "source": [
    "lat_max = 60.0\n",
    "lat_min = 45.0\n",
    "lon_max = 10.0\n",
    "lon_min = 5.0\n",
    "\n",
    "def get_pipeline(v_min, v_max, sort):\n",
    "\n",
    "        min_datetime = '2016-01-01T00:00:00Z'\n",
    "        version = '1'\n",
    "        # gets values of pos between v_min and v_max with timestamp greater than min_datetime\n",
    "        # and with version greater than 1\n",
    "        # and with an address dict\n",
    "        # sort = -1: descending, sort = 1: ascending\n",
    "        pipeline = [{'$unwind': '$pos'},\n",
    "                {'$match': {'$and':\n",
    "                            [{'pos': {'$lt': v_max}}, {'pos': {'$gt': v_min}} ]}},\n",
    "                {'$match': {'created.timestamp': {'$gt': min_datetime}}},\n",
    "                {'$match': {'created.version': {'$gt': version}, 'address': {'$exists': 1} }},   \n",
    "                {'$project': {'pos':'$pos', 'address':'$address'}},\n",
    "                {'$sort': {'pos': sort}}\n",
    "                ]\n",
    "        return pipeline\n",
    "    \n",
    "def get_min_max_coord(v_min, v_max):\n",
    "    \n",
    "    for sort in [-1,1]:\n",
    "        result_list = []\n",
    "        pipeline = get_pipeline(v_min, v_max,sort)\n",
    "        result = db.map_Duesseldorf_Neuss.aggregate(pipeline, allowDiskUse = True )\n",
    "        result_list = [result.next() for i in range(0,1)]\n",
    "       # pprint.pprint(result_list) \n",
    "        \n",
    "        if sort == -1:\n",
    "            print 'The max value {0:.5f}.'.format(result_list[0]['pos'])\n",
    "        else:\n",
    "            print 'The min value {0:.5f}.'.format(result_list[0]['pos'])\n",
    "\n",
    "get_min_max_coord(lat_min, lat_max)\n",
    "get_min_max_coord(lon_min, lon_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The south-north distance is 7.490 km.\n",
      "The west-east distance is 15.520 km.\n",
      "The total area is 116.249 km^2.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_distance(lat_1, lat_2, lon_1, lon_2):\n",
    "# all in km and radians   \n",
    "    coords = np.pi/180.0*np.array([lat_1, lat_2, lon_1, lon_2])\n",
    "    R = 6371.0\n",
    "    \n",
    "    return 2.0*R*np.arcsin(np.sqrt( np.sin(0.5*(coords[1]-coords[0]))**2 + np.cos(coords[0])*np.cos(coords[1])\n",
    "                                   *np.sin(0.5*(coords[3]-coords[2]))**2 )) \n",
    "# five decimal places = 1 m accuracy\n",
    "d_south_north = get_distance(51.16696,51.23432,6.64838,6.64838)\n",
    "d_west_east = get_distance(51.23432,51.23432,6.64838,6.87130)\n",
    "area = d_south_north*d_west_east\n",
    "print 'The south-north distance is {0:.3f} km.'.format(d_south_north)\n",
    "print 'The west-east distance is {0:.3f} km.'.format(d_west_east)\n",
    "print 'The total area is {0:.3f} km^2.'.format(area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many pharmacies are on the map?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of pharmacies on the map is 124.\n",
      "The average number of pharmacies per km^2 is 1.06667351055.\n"
     ]
    }
   ],
   "source": [
    "pipeline = [{'$group': { '_id': '$amenity',\n",
    "                              'count': {'$sum': 1 }  }},\n",
    "             {'$match': {'_id': 'pharmacy'}},\n",
    "            \n",
    "                {'$sort': {'count': -1}}\n",
    "           ]\n",
    "result = db.map_Duesseldorf_Neuss.aggregate(pipeline)\n",
    "result_list = []\n",
    "while result.alive == True:\n",
    "    result_list.append(result.next()) \n",
    "\n",
    "print 'The number of pharmacies on the map is {}.'.format(result_list[0]['count'])\n",
    "print 'The average number of pharmacies per km^2 is {}.'.format(result_list[0]['count']/area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many amenties have an access for a wheelchair and which ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{u'_id': None, u'count': 968150},\n",
      " {u'_id': u'yes', u'count': 1673},\n",
      " {u'_id': u'no', u'count': 977},\n",
      " {u'_id': u'limited', u'count': 805},\n",
      " {u'_id': u'unknown', u'count': 19}]\n",
      "\n",
      "\n",
      " 48.16 % of the amenities with a wheelchair value have a wheelchair access.\n",
      " 28.12 % of the amenities with a wheelchair value don't have a wheelchair access.\n",
      "For  23.17 % of the amenities with a wheelchair value the wheelchair access is limited.\n",
      "For  0.55 % of the amenities with a wheelchair value the wheelchair access is unknown.\n"
     ]
    }
   ],
   "source": [
    "pipeline = [{'$group': { '_id': '$wheelchair',\n",
    "                              'count': {'$sum': 1 }  }},            \n",
    "                {'$sort': {'count': -1}}\n",
    "           ]\n",
    "result = db.map_Duesseldorf_Neuss.aggregate(pipeline)\n",
    "result_list = []\n",
    "while result.alive == True:\n",
    "    result_list.append(result.next()) \n",
    "    \n",
    "pprint.pprint(result_list[:])\n",
    "elem_sum = 0.0\n",
    "for elem in result_list[1:]:\n",
    "    elem_sum = elem_sum + elem['count']\n",
    "print '\\n'\n",
    "print '{0: .2f} % of the amenities with a wheelchair value have a wheelchair access.'.format(100.0*result_list[1]['count']/elem_sum) \n",
    "print '{0: .2f} % of the amenities with a wheelchair value don\\'t have a wheelchair access.'.format(100.0*result_list[2]['count']/elem_sum)\n",
    "print 'For {0: .2f} % of the amenities with a wheelchair value the wheelchair access is limited.'.format(100.0*result_list[3]['count']/elem_sum)\n",
    "print 'For {0: .2f} % of the amenities with a wheelchair value the wheelchair access is unknown.'.format(100.0*result_list[4]['count']/elem_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The city should definitely do more for the the wheelchair users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tope ten of amenities with a wheelchair access.\n",
      "[{u'_id': u'parking', u'count': 98},\n",
      " {u'_id': u'pharmacy', u'count': 49},\n",
      " {u'_id': u'restaurant', u'count': 48},\n",
      " {u'_id': u'bank', u'count': 47},\n",
      " {u'_id': u'fast_food', u'count': 46},\n",
      " {u'_id': u'toilets', u'count': 42},\n",
      " {u'_id': u'cafe', u'count': 25},\n",
      " {u'_id': u'pub', u'count': 20},\n",
      " {u'_id': u'fuel', u'count': 14},\n",
      " {u'_id': u'post_office', u'count': 14}]\n"
     ]
    }
   ],
   "source": [
    "pipeline = [{'$match': { 'wheelchair': 'yes', 'amenity':{'$exists':1}}},\n",
    "            {'$group': { '_id': '$amenity',\n",
    "                       'count': {'$sum': 1 }}},\n",
    "            {'$sort': {'count': -1}}\n",
    "           ]\n",
    "result = db.map_Duesseldorf_Neuss.aggregate(pipeline)\n",
    "result_list = []\n",
    "while result.alive == True:\n",
    "    result_list.append(result.next()) \n",
    "\n",
    "print 'Top ten of amenities with a wheelchair access.'     \n",
    "pprint.pprint(result_list[0:10])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='red'>   Suggestions for improving and analyzing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> I see at least three problems in and with the dataset. The first one is that the values inserted by the OSM users are not standardized, which results in an undesired variety of data formats. During the data cleaning process, for instance, I saw different formats of phone data: '0049 2131 775239', '+49 2131 775239', '+49 (0)2131 775239' or '02131-775239'. Also the way how e.g. opening times are stored is rather messy. Hier an example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<node id=\"4024283014\" lat=\"51.2012348\" lon=\"6.6914372\" version=\"1\" timestamp=\"2016-02-24T20:53:45Z\" changeset=\"37421800\" uid=\"141931\" user=\"Sharlin\">\n",
    "    <tag k=\"addr:city\" v=\"Neuss\"/>\n",
    "    <tag k=\"addr:country\" v=\"DE\"/>\n",
    "    <tag k=\"addr:housenumber\" v=\"18\"/>\n",
    "    <tag k=\"addr:postcode\" v=\"41460\"/>\n",
    "    <tag k=\"addr:street\" v=\"RheinstraÃŸe\"/>\n",
    "    <tag k=\"contact:email\" v=\"verkehrslenkung@stadt.neuss.de\"/>\n",
    "    <tag k=\"contact:fax\" v=\"+49 2131 90-2490\"/>\n",
    "    <tag k=\"contact:phone\" v=\"+49 2131 90-3901\"/>\n",
    "    <tag k=\"contact:website\" v=\"http://www.neuss.de/\"/>\n",
    "    <tag k=\"name\" v=\"Amt fÃ¼r Verkehrsangelegenheiten\"/>\n",
    "    <tag k=\"office\" v=\"administrative\"/>\n",
    "    <tag k=\"opening_hours\" v=\"Mo-We 08:00-16:00; Tu 08:00-18:00; Fr 08:00-12:30\"/>\n",
    "    <tag k=\"operator\" v=\"Stadt Neuss\"/>\n",
    "    <tag k=\"ref\" v=\"69\"/>\n",
    "  </node>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> To sove this problem, I think, more standardization from the side of OSM , especially for the values, is required. The second problem that I see are many fixme keys. Some issues like inserting relations, opening hours and bus stations can be fixed sitting behind the computer by internet search. Other issues like exact GPS coordinates require in-the-field activity (i.e. going to the place and measureing the GPS coordinates).\n",
    "The third problem is checking the validity of the data, e.g. the the validity of existing GPS coordinates. This require user engagement, which, as pointed out in the sample project, can be stimulated by gamification elements. On the other hand, GPS traces from e.g. Pokemon Go'ers, hobby runners and bikers, vehicle fleets etc. can be potentially used to enrich and cross-check the existent dataset. Also Google Maps can be used as a reference though an automated cross-check with Google Maps is difficult, I believe, because the data are not available as e.g. XML files. Here is the summary of my reasoning.                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> __Problem: values inserted by users are not standerized__\n",
    "\n",
    "__Solution: standarization of input values by OSM__\n",
    "\n",
    "__Benefits__:\n",
    "\n",
    "- better for database queries\n",
    "\n",
    "__Anticipated Problems__:\n",
    "\n",
    "- not all cases can be covered by standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> __Problem: large amount of fixme keys__\n",
    "\n",
    "__Solution: __internet search, in-the-field activity\n",
    "\n",
    "__Benefits__:\n",
    "\n",
    "- improves the dataset quality\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> __Problem: Are the present OSM data correct?__\n",
    "\n",
    "__Solution: user audits (motivation with gamification elements), GPS traces from different soures (e.g. vehicle fleets) and Google Map data for validation of present OSM data __\n",
    "\n",
    "\n",
    "__Benefits__:\n",
    "\n",
    "- improves the dataset quality\n",
    "\n",
    "__Anticipated Problems__:\n",
    "\n",
    "- requires user engagament\n",
    "- legal issues (privacy, licences, etc.)\n",
    "- technical feasibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<node id=\"4024283014\" lat=\"51.2012348\" lon=\"6.6914372\" version=\"1\" timestamp=\"2016-02-24T20:53:45Z\" changeset=\"37421800\" uid=\"141931\" user=\"Sharlin\">\n",
    "    <tag k=\"addr:city\" v=\"Neuss\"/>\n",
    "    <tag k=\"addr:country\" v=\"DE\"/>\n",
    "    <tag k=\"addr:housenumber\" v=\"18\"/>\n",
    "    <tag k=\"addr:postcode\" v=\"41460\"/>\n",
    "    <tag k=\"addr:street\" v=\"RheinstraÃŸe\"/>\n",
    "    <tag k=\"contact:email\" v=\"verkehrslenkung@stadt.neuss.de\"/>\n",
    "    <tag k=\"contact:fax\" v=\"+49 2131 90-2490\"/>\n",
    "    <tag k=\"contact:phone\" v=\"+49 2131 90-3901\"/>\n",
    "    <tag k=\"contact:website\" v=\"http://www.neuss.de/\"/>\n",
    "    <tag k=\"name\" v=\"Amt fÃ¼r Verkehrsangelegenheiten\"/>\n",
    "    <tag k=\"office\" v=\"administrative\"/>\n",
    "    <tag k=\"opening_hours\" v=\"Mo-We 08:00-16:00; Tu 08:00-18:00; Fr 08:00-12:30\"/>\n",
    "    <tag k=\"operator\" v=\"Stadt Neuss\"/>\n",
    "    <tag k=\"ref\" v=\"69\"/>\n",
    "  </node>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<node id=\"4024283014\" lat=\"51.2012348\" lon=\"6.6914372\" version=\"1\" timestamp=\"2016-02-24T20:53:45Z\" changeset=\"37421800\" uid=\"141931\" user=\"Sharlin\">\n",
    "    <tag k=\"addr:city\" v=\"Neuss\"/>\n",
    "    <tag k=\"addr:country\" v=\"DE\"/>\n",
    "    <tag k=\"addr:housenumber\" v=\"18\"/>\n",
    "    <tag k=\"addr:postcode\" v=\"41460\"/>\n",
    "    <tag k=\"addr:street\" v=\"RheinstraÃŸe\"/>\n",
    "    <tag k=\"contact:email\" v=\"verkehrslenkung@stadt.neuss.de\"/>\n",
    "    <tag k=\"contact:fax\" v=\"+49 2131 90-2490\"/>\n",
    "    <tag k=\"contact:phone\" v=\"+49 2131 90-3901\"/>\n",
    "    <tag k=\"contact:website\" v=\"http://www.neuss.de/\"/>\n",
    "    <tag k=\"name\" v=\"Amt fÃ¼r Verkehrsangelegenheiten\"/>\n",
    "    <tag k=\"office\" v=\"administrative\"/>\n",
    "    <tag k=\"opening_hours\" v=\"Mo-We 08:00-16:00; Tu 08:00-18:00; Fr 08:00-12:30\"/>\n",
    "    <tag k=\"operator\" v=\"Stadt Neuss\"/>\n",
    "    <tag k=\"ref\" v=\"69\"/>\n",
    "  </node>\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
